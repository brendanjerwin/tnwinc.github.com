---
layout: post
title: "Decomposing a Testing Approach"
tags: [testing, QA]
author: Doug Lewis
mail: douglewis@tnwinc.com
published: true
summary: "Understanding a complete testing approach requires decomposing the problem."
---
{% include JB/setup %}

Here at the labs, the QA team is undergoing some pretty major overhaul as we prepare to test our company’s upcoming projects.  As we’ve been kicking around tools to try out, I’ve been running them by the developers and we’ve had some pretty lively discussions as a result.  Coming into one particular one, I thought I was trying to answer the question “What’s the best way to test an application?”  The developer I was talking to was also trying to answer that question but we had a hard time coming to common ground because it felt like we were actually trying to answer different questions.  With some more discussion, we realized that “What’s the best way to test an application?” is really too general to act upon.  Understanding our test approach makes a lot more sense when it is decomposed a bit:

## How can we best evaluate the customer’s experience?

Software testing usually begins with an effort to test the system by carrying out the same behaviors a tester expects the customer will follow.  One of the main reasons for beginning a test approach here is that although the user experience is only a fraction of the application from a code perspective, it is the entire application from the user’s perspective.  Many test cases address imagined support calls and customer complaints and step through them ahead of time to make sure they don’t surface in production.  A customer may not know if there are problems with data storage as opposed to business logic, but they’ll definitely notice and report a problem in the context of “I clicked ‘New’, entered my data and clicked ‘Save’, I then went to the report, but it doesn’t show my record on it.”
Traditionally, this process has involved creating a large number of manual test cases to mimic the customer’s actions.  The advantage of these test cases is that, individually, they’re easy to write and change as requirements change and they don’t require any technical expertise to author or maintain.  They can be generated by numerous team members, even outside the development team and could theoretically be run by just about anyone.  Finally, they can be generated while the code is still in development (assuming the tester and the developer are on the same page about how the application will work).
As a testing effort grows, manual cases can become unwieldy.  Time is a valuable resource and running a deep regression of hundreds or thousands of manual cases becomes unfeasible, especially in an Agile development cycle that emphasizes a fast turnaround.  The next growth step consists of upgrading some cases with an automatic playback.  A tester can use a recorder to step through the case manually once and then launch the recording for any subsequent runs.  The runs of the test cases speed up, since the recording can click faster than a person and the results are more reliable (assuming they were recorded correctly to begin with).  
Playback recordings also hit some limits as they grow in scale.  Any change in the test will require a re-recording and most playback tools offer little to no functionality for data validation. They also often require the tester to go in and run them one at a time. The next upgrade these test cases can go through takes them up to a Coded UI test.  This allows a tester with programming experience to inject additional code to perform steps a playback mechanism can’t do on its own.  Coded UI tests are typically run in a harness that will allow a large group of tests to be run in the background.  They’re a significantly more expensive investment then a manual test, but they also offer a much higher return over time.
Currently, we’re taking several approaches to tackling this problem.  For manual test case tracking we’re experimenting with [Visual Studio Test Professional 2010](http://www.microsoft.com/visualstudio/en-us/products/2010-editions/test-professional).  Test Professional also offers recording and playback functionality, as well as a simple method for exporting playbacks to Visual Studio Premium to create C# based coded UI tests.  In addition to evaluating those tools, we have some Coded UI automation already completed using [Selenium](http://seleniumhq.org) for recording and playback, Java for the language of the tests and Bromine to manage them.

## What are the best ways to test the system fully and deeply?

One of the areas where this paradigm of mimicking the customer experience doesn’t shine is when you want to get into deeper tests of the system logic.  Even a large suite of coded UI tests can pose some major problems if you’ve got hundreds or thousands of ways of manipulating a feature and you really want to test them all out.  The first problem is speed.  The act of spinning up a UI and manipulating controls to work through test conditions one at a time may prove very inefficient if your application can handle and respond to multiple requests at once.  This can be really painful if you’d like to run a large test suite automatically with each build that development produces.  A second problem is brittleness.  UI changes can easily break Coded UI tests and your overall testing effort can be really hampered if you were depending on large volumes of those tests to evaluate non-UI functionality (like business logic or data storage).  They present one final challenge to a fast moving Agile team:  they can’t be built until the UI changes are completed, built and delivered to QA.
These problems represent a big technical challenge and one that most off-the-shelf testing suites are ill equipped to address.  One of the conclusions we’ve reached is that there’s no way around this problem without wading into the world of coding.  Some of the team members here are beginning to work with [SpecFlow](http://specflow.org) to see if we can leverage it for building better tests while code is still in development.  We’ll definitely be due for another blog post once we’ve had some time to try and integrate it.

## How can we make sure the application performs acceptably in real-world conditions?

An application needs to perform well under production conditions.  Users may not appreciate an application’s many features if they are spending lots of time staring at progress bars to get to them (or receiving errors that the website is down).  For QA, this means covering a pretty large checklist:

 *  Generating a test environment that represents a meaningful chunk of our production environment

 * Identifying a tool for running load and stress tests

 * Identifying our benchmarks (figuring out the dozens to hundreds of little metrics that add up to “good” performance)

 * Building up a meaningful test suite of user behavior that we believe will occur in a large volume

 * Running tests at expected loads for our customer base to make sure the production system can handle it and still give good performance

 * Running heavy enough loads to introduce failures in order to assess how gracefully the application fails (Does it automatically recover when the load drops?  Does it lose data during a failure? Etc…)

One of the big challenges in this task has been selecting a load testing tool.  Most professional tools can run anywhere from $10,000 to over $100,000 depending on your usage.  We’re currently experimenting with an open source tool, [Apache JMeter](http://jmeter.apache.org/), with some pretty good results.  It doesn’t have some of the features that the professional tools do, but it’s also free.  As we get more comfortable with this process, we can always circle back around and see if a tool like Visual Studio Ultimate or Webload can actually justify the cost for us.

